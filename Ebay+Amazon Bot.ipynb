{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7161435",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install selenium-wire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8257b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import threading\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from seleniumwire import webdriver\n",
    "from collections import deque\n",
    "import itertools\n",
    "file_name = \"sample.txt\"\n",
    "\n",
    "options = {\n",
    "    'proxy': {\n",
    "        'https': '',\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "proxies=deque(['Use your custom proxies here'])\n",
    "\n",
    "\n",
    "\n",
    "# # Open the file for reading\n",
    "# with open(file_name, \"r\") as file:\n",
    "#     # Read the contents of the file\n",
    "#     for line in file:\n",
    "#         line=line.strip()\n",
    "#         proxies.append(line)\n",
    "    \n",
    "proxy_cycle = itertools.cycle(proxies)\n",
    "initial_proxy = next(proxy_cycle)\n",
    "options['proxy']['https']='https://'+initial_proxy\n",
    "\n",
    "\n",
    "\n",
    "def change_proxy(index):\n",
    "    thread_ip = proxies[index % len(proxies)]\n",
    "    tt='http://'+thread_ip\n",
    "    print(\"Proxy used: \",tt)\n",
    "    print(\"\\n\")\n",
    "    options1= {\n",
    "    'proxy': {\n",
    "        'https': tt,\n",
    "        \n",
    "    }\n",
    "}\n",
    "    return webdriver.Chrome(seleniumwire_options=options1)\n",
    "\n",
    "\n",
    "target_titles=input(\"Enter the title amount that needs to be scrapped for the specific url: \")\n",
    "target_titles=int(target_titles)\n",
    "target_uri=input(\"Please copy paste the url of the target ebay webpage product: \")\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open a website\n",
    "\n",
    "def Ebay_loader(target_uri):\n",
    "    driver.get(target_uri)\n",
    "    time.sleep(4)\n",
    "    button = element = driver.find_element(By.XPATH, '/html/body/div[5]/div[4]/div[1]/div[2]/div[1]/div[3]/div[2]/span/span/button/span')\n",
    "    button.click()\n",
    "    time.sleep(2)\n",
    "    button = element = driver.find_element(By.XPATH,'//*[@id=\"s0-53-17-5-4[1]-74-40-2-content-menu\"]/li[2]/button/span')\n",
    "    button.click()\n",
    "    time.sleep(4)\n",
    "\n",
    "    button=element = driver.find_element(By.XPATH,'//*[@id=\"cust-ipp-4\"]')\n",
    "    button.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    button=element = driver.find_element(By.XPATH,'/html/body/div[7]/div[2]/div[3]/div/form/div[2]/button[3]')\n",
    "    button.click()\n",
    "    time.sleep(2)\n",
    "# Close the browser\n",
    "#driver.quit()\n",
    "\n",
    "\n",
    "    button=element = driver.find_element(By.XPATH,'/html/body/div[5]/div[4]/div[1]/div[2]/div[2]/div/div[1]/h1')\n",
    "    total_results=button.text\n",
    "    int_results=total_results.split()[0]\n",
    "    if \"+\" in int_results:\n",
    "        temp=int_results\n",
    "        temp=temp.split(\"+\")\n",
    "        int_results=temp[0].replace(',','')\n",
    "        \n",
    "    int_results=int(int_results)\n",
    "    int_results+=2\n",
    "    \n",
    "    return int_results\n",
    "\n",
    "\n",
    "def next_page_shift(count,int_results,Titles_list):\n",
    "    \n",
    "    for i in range(2,243):\n",
    "        if count<int_results:\n",
    "            if i%241!=0:\n",
    "            \n",
    "                element = driver.find_element(By.XPATH,f\"/html/body/div[5]/div[4]/div[2]/div[1]/div[2]/ul/li[{i}]/div/div[2]/a/div/span\")\n",
    "                Titles_list.append(element.text)\n",
    "                \n",
    "                count+=1\n",
    "            else:\n",
    "                print(\"\\n Crawling to next page hehehe\")\n",
    "                element= driver.find_element(By.XPATH,\"/html/body/div[5]/div[4]/div[2]/div[1]/div[2]/ul/li[242]/div[2]/span/span/nav/a\")\n",
    "                element.click()\n",
    "            \n",
    "                time.sleep(3)\n",
    "                count+=1\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return count,Titles_list\n",
    "\n",
    "\n",
    "int_results=Ebay_loader(target_uri)\n",
    "count=2\n",
    "\n",
    "Titles_list=[]\n",
    "while count<int_results:\n",
    "    count,Titles_List=next_page_shift(count,int_results,Titles_list)\n",
    "\n",
    "print(len(Titles_list))\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "# Find the search input element\n",
    "\n",
    "\n",
    "threads=[]\n",
    "Parent_sub_categories=[]\n",
    "\n",
    "num_threads = 3\n",
    "\n",
    "# Calculate the number of URLs per chunk\n",
    "\n",
    "HREF=[]\n",
    "\n",
    "def Threading_for_Amazon(chunk,index):\n",
    "    \n",
    "    chunkk=chunk\n",
    "    indd=index\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        for url in chunk:\n",
    "\n",
    "            ASNN=\"\"\n",
    "            driver1 =change_proxy(indd)\n",
    "            query=url+\" Amazon us\"\n",
    "            url=f'https://www.google.com/search?q={query}'\n",
    "            driver1.get(url)\n",
    "            time.sleep(2)\n",
    "\n",
    "            first_result = driver1.find_element(By.CSS_SELECTOR, 'h3')\n",
    "            first_result.click()\n",
    "\n",
    "            current_url = driver1.current_url\n",
    "            current_url=current_url.split(\"/\")\n",
    "            ASN=current_url[5]\n",
    "            ASN=ASN.split(\"?\")\n",
    "\n",
    "\n",
    "            Parent_ASN_List.append(ASN[0])\n",
    "            print(\"Parent ASN: \",ASN[0])\n",
    "            ASNN=ASN[0]\n",
    "\n",
    "            span_xpath = 'a-list-item'\n",
    "            span_xpath1 = 'span.a-list-item'\n",
    "\n",
    "            span_xpath = f'//span[@class=\"{span_xpath}\"]/a'\n",
    "\n",
    "    # Find the first <span> element using the constructed XPath\n",
    "            first_span = driver1.find_elements(By.XPATH,span_xpath)\n",
    "\n",
    "            \n",
    "            hreff=[]\n",
    "            for span in first_span:\n",
    "                hreff = span.get_attribute('href')\n",
    "            \n",
    "            \n",
    "            first_span = driver1.find_elements(By.CSS_SELECTOR,span_xpath1)\n",
    "\n",
    "            subcategory=\"\"\n",
    "            categories=[]\n",
    "            for span in first_span:\n",
    "            \n",
    "                categories.append(span.text)\n",
    "    \n",
    "    \n",
    "            element_to_find = 'â€º'\n",
    "\n",
    "# Use slicing to find the last index of 3 in the list\n",
    "            try:\n",
    "                last_index = len(categories) - categories[::-1].index(element_to_find) - 1\n",
    "            except ValueError:\n",
    "                last_index = -1  # Element not found\n",
    "\n",
    "            subcategory=categories[last_index+1]\n",
    "            print(\"Parent Subcategory: \", subcategory)\n",
    "            Parent_sub_categories.append(subcategory)\n",
    "\n",
    "            time.sleep(2)\n",
    "            \n",
    "            driver1=change_proxy(indd)\n",
    "            \n",
    "            print(\"Loading Subcategory page.....!\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            driver1.get(hreff[len(hreff)-2])\n",
    "            \n",
    "            time.sleep(2)\n",
    "\n",
    "            a_elements = driver1.find_elements(\"tag name\",'a')\n",
    "\n",
    "            Href=[]\n",
    "            titles=[]\n",
    "    # Extract titles and href attributes\n",
    "            for a_element in a_elements:\n",
    "                title = a_element.text\n",
    "                href = a_element.get_attribute('href')\n",
    "                if title and href:\n",
    "                    temp=title\n",
    "                    temp=temp.split(\" \")\n",
    "                    for name in temp:\n",
    "                        if name in href:\n",
    "                            Href.append(href)\n",
    "                            break\n",
    "\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "\n",
    "\n",
    "            sub_ASIN=[]\n",
    "            for url in set(Href):\n",
    "                temp=url\n",
    "                temp=temp.split(\"/\")\n",
    "                try:\n",
    "                    ASIN=temp[5]\n",
    "                    if len(ASIN)==10:\n",
    "                        sub_ASIN.append(ASIN)\n",
    "\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            Sub_final_ASN_List.append(sub_ASIN)\n",
    "\n",
    "            if len(sub_ASIN)>1:\n",
    "                Parent_sub_ASN[ASNN]=sub_ASIN\n",
    "                #rint(Parent_sub_ASN)\n",
    "\n",
    "            else:\n",
    "                print(\"No sufficient Barcodes to extract.....\")\n",
    "\n",
    "                driver1.quit()\n",
    "        \n",
    "    \n",
    "    except Exception:\n",
    "        driver1.quit()\n",
    "        print(\"Expected Amazon captcha or other error, trying again!.....\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        Threading_for_Amazon(chunkk,indd+1)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "Parent_ASN_List=[]\n",
    "sub_ASN_List=[]\n",
    "Sub_final_ASN_List=[]\n",
    "tt=[]\n",
    "\n",
    "Parent_sub_ASN={}\n",
    "\n",
    "# Enter your search query\n",
    "threads = []\n",
    "\n",
    "urls_per_chunk = target_titles // num_threads\n",
    "\n",
    "for i in range(num_threads):\n",
    "    updated_uri=[]\n",
    "    updated_uri=Titles_list[:target_titles]\n",
    "    start = i * urls_per_chunk\n",
    "    end = start + urls_per_chunk if i < num_threads - 1 else None\n",
    "    chunk = updated_uri[start:end]\n",
    "\n",
    "    thread = threading.Thread(target=Threading_for_Amazon,args=(chunk,i))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "# This part of the code will execute after all threads are done\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "keys = list(Parent_sub_ASN.keys())\n",
    "values = list(Parent_sub_ASN.values())\n",
    "\n",
    "csv_dict=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    key = keys[i]\n",
    "    value = values[i]\n",
    "    \n",
    "    print(key,\"   \", Parent_sub_categories[i],\"   \", value)\n",
    "    \n",
    "    \n",
    "print(\"All web pages have been loaded.\")    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
